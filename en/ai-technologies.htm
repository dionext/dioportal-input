<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta content="en-us" http-equiv="Content-Language" />
<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
<title>Untitled 1</title>
</head>

<body>

<h1>AI. Technologies</h1>
<h3>&nbsp;Low-rank adaptation (LoRA)</h3>
<p>
For additional training, there is, for example, a popular one
<a href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a>&nbsp; 
But you need to be able to deploy it, and also still understand the dataset 
formats.
So far, the industry is not very standardized and there are no solutions for 
Win/Mac like office packages.</p>
<p>
<a href="https://habr.com/ru/articles/776872/">https://habr.com/ru/articles/776872/</a> 
(ru) In this guide, author tell and show you how to additionally train the 
7-billionth Saiga model by Ilya Gusev @Takagi for your tasks.
This article is a kind of tutorial on additional training of a model on Kaggle.</p>
<h3><a name="id9"></a>Embedding</h3>
<p><a href="https://www.youtube.com/watch?v=F0FGcxkMElk">https://www.youtube.com/watch?v=F0FGcxkMElk</a> 
Generating Embeddings with Spring AI: Step-by-Step Guide with Pitfalls </p>
<h4><a name="id53"></a>Choose Embedding model</h4>
<p>
<a href="https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/">
https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/</a>&nbsp;&nbsp;
<a href="https://huggingface.co/spaces/mteb/leaderboard">https://huggingface.co/spaces/mteb/leaderboard</a>
<a href="https://www.pinecone.io/learn/series/rag/embedding-models-rundown/">
https://www.pinecone.io/learn/series/rag/embedding-models-rundown/</a> </p>
<ul>
	<li>
	Max Tokens: The number of tokens that can be squeezed into a single 
	embedding.
	Typically, you won’t want to fit more than a paragraph of text (~100 tokens) 
	into a single embedding.
	So even embedding models with a max token count of 512 should be more than 
	enough. </li>
	<li>
	Embedding Dimensions: The length of the embedding vector.
	Smaller embeddings allow for faster inference and are more storage 
	efficient, while more dimensions can capture fine details and relationships 
	in the data.
	Ultimately, we want to find a good tradeoff between capturing data 
	complexity and operational efficiency.</li>
</ul>
<ul>
	<li>
	<div>
		<div>
			<a data-track="true" href="https://docs.voyageai.com/embeddings/" tabindex="0" target="_blank">
			<span>voyage-lite-02-instruct</span></a><span>: A proprietary 
			embedding model from VoyageAI</span></div>
	</div>
	</li>
	<li>
	<div>
		<div>
			<a data-track="true" href="https://openai.com/blog/new-embedding-models-and-api-updates" tabindex="0" target="_blank">
			<span>text-embedding-3-large</span></a><span>: One of OpenAI’s 
			latest proprietary embedding models</span></div>
	</div>
	</li>
	<li>
	<div>
		<div>
			<a data-track="true" href="https://huggingface.co/WhereIsAI/UAE-Large-V1" tabindex="0" target="_blank">
			<span>UAE-Large-V1</span></a><span>: A small-ish (335M parameters) 
			open-source embedding model</span></div>
	</div>
	</li>
</ul>
<h4><a name="id54"></a>Spliting</h4>
<p>
How to split text for embedding?
The answer depends on each specific domain.
Your main options are: </p>
<ul>
	<li>
	split into paragraphssplit </li>
	<li>
	into sentencessplit </li>
	<li>
	into N words, letters or tokens. </li>
</ul>
<p>
You can also use a sliding window approach, where each chunk includes part of 
the previous chunk. More advanced embedding calculation options: </p>
<ul>
	<li>Summarize chunks</li>
	<li>Extract possible questions or keywords using LLM</li>
</ul>
<h3>Pinecone</h3>
<p><a href="https://www.pinecone.io/pricing/">https://www.pinecone.io/pricing/</a>
</p>
Build knowledgeable AI<p>With its vector database at the core, Pinecone is the 
leading knowledge platform for building accurate, secure, and scalable AI 
applications.</p>
<p>&nbsp;</p>
<h3><a name="id50"></a>RAG</h3>
<p>todo <a href="https://www.youtube.com/watch?v=JanB50ALQg0">https://www.youtube.com/watch?v=JanB50ALQg0</a> 
Иван Насонов | Advanced RAG Pipelines </p>
<p><a href="https://www.youtube.com/watch?v=6Pgmr7xMjiY">https://www.youtube.com/watch?v=6Pgmr7xMjiY</a> 
Java + RAG: Create an AI-Powered Financial Advisor using Spring AI </p>
<p><a href="https://www.youtube.com/watch?v=JeqAdqBA_Yo">https://www.youtube.com/watch?v=JeqAdqBA_Yo</a> 
Загружаем в ИИ большие данные (книги, документы), заставляем ИИ цитировать 
источники, автономия </p>
<p><a href="https://www.youtube.com/watch?v=NArkdDqt-F0">
https://www.youtube.com/watch?v=NArkdDqt-F0</a> Анатомия RAG: Разбираем по 
косточкам современных чат-ботов. Галымжан Туткушев </p>
<p><a href="https://www.baeldung.com/spring-ai-redis-rag-app">
https://www.baeldung.com/spring-ai-redis-rag-app</a> </p>
<p><a href="https://www.youtube.com/watch?v=FA7oLqzz8gQ&amp;t=16s">
https://www.youtube.com/watch?v=FA7oLqzz8gQ&amp;t=16s</a> Advanced RAG: Combining 
RAG with Text-to-SQL - LlamaIndex </p>
<h4><a name="id56"></a>NaiveRAG</h4>
<p>Naive RAG is a paradigm that combines information retrieval with natural 
language generation to produce responses to queries or prompts. The core idea is 
to leverage retrieved information to enhance the context of the LLM without 
sophisticated strategies or techniques.
<a href="https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag">
https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag</a>
</p>
<h4><a name="id57"></a>RQ-RAG</h4>
<p><a href="https://github.com/chanchimin/RQ-RAG">
https://github.com/chanchimin/RQ-RAG</a> </p>
<p><a href="https://aiexpjourney.substack.com/p/a-brief-introduction-to-rq-rag">
https://aiexpjourney.substack.com/p/a-brief-introduction-to-rq-rag</a> </p>
<h4><a name="id58"></a>Hypothetical Document Embeddings(HyDE)</h4>
<p>Advanced RAG — Improving retrieval using Hypothetical Document 
Embeddings(HyDE) </p>
<p>HyDE uses a Language Learning Model, like ChatGPT, to create a theoretical 
document when responding to a query, as opposed to using the query and its 
computed vector to directly seek in the vector database. It goes a step further 
by using an unsupervised encoder learned through contrastive methods. This 
encoder changes the theoretical document into an embedding vector to locate 
similar documents in a vector database. Rather than seeking embedding similarity 
for questions or queries, it focuses on answer-to-answer embedding similarity. 
Its performance is robust, matching well-tuned retrievers in various tasks such 
as web search, QA, and fact verification
<a href="https://medium.aiplanet.com/advanced-rag-improving-retrieval-using-hypothetical-document-embeddings-hyde-1421a8ec075a">
https://medium.aiplanet.com/advanced-rag-improving-retrieval-using-hypothetical-document-embeddings-hyde-1421a8ec075a</a>
</p>
<h4><a name="id59"></a>GraphRAG</h4>
<p>GraphRAG is a structured, hierarchical approach to Retrieval Augmented 
Generation (RAG), as opposed to naive semantic-search approaches using plain 
text snippets. The GraphRAG process involves extracting a knowledge graph out of 
raw text, building a community hierarchy, generating summaries for these 
communities, and then leveraging these structures when perform RAG-based tasks.</p>
<p><a href="https://microsoft.github.io/graphrag/">https://microsoft.github.io/graphrag/</a>
</p>
<p>
<a href="https://medium.com/@zilliz_learn/graphrag-explained-enhancing-rag-with-knowledge-graphs-3312065f99e1">
https://medium.com/@zilliz_learn/graphrag-explained-enhancing-rag-with-knowledge-graphs-3312065f99e1</a>
</p>
<p><a href="https://habr.com/ru/articles/857354/">https://habr.com/ru/articles/857354/</a>
</p>
<h4><a name="id60"></a>LightRAG</h4>
<p><a href="https://lightrag.github.io/">https://lightrag.github.io/</a> (
<a href="https://arxiv.org/pdf/2410.05779v2">https://arxiv.org/pdf/2410.05779v2</a> 
)</p>
<p><a href="https://github.com/HKUDS/LightRAG">https://github.com/HKUDS/LightRAG</a>
</p>
<p><a href="https://www.youtube.com/watch?v=oageL-1I0GE">https://www.youtube.com/watch?v=oageL-1I0GE</a> 
LightRAG: A More Efficient Solution than GraphRAG for RAG Systems? </p>
<p><a href="https://www.youtube.com/watch?v=5EmRZcJIfnw&amp;t=11s">https://www.youtube.com/watch?v=5EmRZcJIfnw&amp;t=11s</a> 
LightRAG - A simple and fast RAG that beats GraphRAG? (paper explained) </p>
<h3 id="what-is-re-reading-re2"><a name="id118"></a>Re-reading (RE2)</h3>
<p>Sometimes<span>&nbsp;</span><a href="https://learnprompting.org/docs/vocabulary/LLM" rel="noopener noreferrer" target="_blank">Large 
Language Models (LLMs)</a><span>&nbsp;</span>just fail to notice important details in 
our prompt and this leads to incorrect results.</p>
<p>Re-reading (RE2)<span><a href="https://learnprompting.org/docs/advanced/zero_shot/re_reading?srsltid=AfmBOoqNe82xsNUkmxsh_P0B2928FUWwRMi_6yuJ2c18Lo9EmmENAxTU#footnotes">1</a>Xu, 
X., Tao, C., Shen, T., Xu, C., Xu, H., Long, G., &amp; guang Jian-Lou. (2024). 
Re-Reading Improves Reasoning in Large Language Models.&nbsp;<a href="https://arxiv.org/abs/2309.06275" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/2309.06275</a>
</span>is a technique that aims to solve this problem by asking an LLM to 
re-read the prompt.</p>
<div>
	<div>
		<p>RE2 Prompting</p>
	</div>
	<div>
		<p>Q: {Input Query} Read the question again: {Input Query}</p>
	</div>
</div>
<p>While being so simple, RE2 is a general end effective technique that 
consistently enhances the reasoning performance of LLMs through just a rereading 
strategy. Furthermore, RE2 is compatible and can be combined with most 
thought-eliciting prompting methods, including<span>&nbsp;</span><a href="https://learnprompting.org/docs/intermediate/chain_of_thought" rel="noopener noreferrer" target="_blank">Chain-of-Thought 
(CoT)</a>.</p>
<p></p>

</body>

</html>
